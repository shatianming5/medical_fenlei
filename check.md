下面给你一个“可以直接落地”的完整方案，目标是：在 **样本有限 + 类别极不均衡 + 可能有噪声/域差** 的前提下，把结果做得 **稳定、可解释、可复现**，并且对你当前更关心的 **二分类临床对比任务** 提供一套统一的训练/评估框架（同时也给出 6 分类的可行替代路线）。

---

## 0. 先把“成功标准”写死（否则永远在调参）

你要的不是“某次跑出来 AUROC 很高”，而是：

1. **稳定**

* 同一设置不同随机种子（≥3 个 seed）指标波动小
* 验证集上给出 **置信区间**（尤其是极少正例任务）

2. **可解释**

* 模型能指出关键切片/区域（至少 slice-level 的解释 + CAM/attention）

3. **可复现**

* 数据清单（manifest）、预处理参数、训练配置、seed、代码版本一一对应
* 同样输入→同样输出（在可控范围内）

> 你后面所有工程/算法选择都围绕这三条服务。

---

## 1. 数据治理：先把“数据和切分”变成可审计的资产

### 1.1 建立统一 manifest（强烈建议）

为每个“耳朵样本”建一行（耳朵作为样本单位更适合二分类）：

* `exam_id, patient_id(若有), series_uid, side(L/R), label_code(1..6), label_id(0..5), has_label, dicom_path, spacing(x,y,z), thickness, n_instances, date_match_flag, vendor/kernel/...`
* 额外派生字段：`is_normal`, `is_cholesteatoma`, `is_code4`, `is_abnormal`, `task_valid_mask(针对每个二分类task是否纳入)` 等

**好处：**

* 任何任务只是“筛表+映射标签”，不会改数据底层逻辑
* 你可以非常清晰地做数据 QA、分布对比、泄漏检查、复现实验

### 1.2 切分策略（避免隐性泄漏）

你现在是 exam 级别 split（且有固定验证集），这很好，但要额外做两件事：

* **病人级泄漏检查**（如果能拿到 `patient_id/MRN`）：同一病人的多次检查不能跨 train/val

  * 如果无法拿到 patient_id：至少用 `study_date + name hash` 等弱标识做近似去重检查（也写进 manifest）

* **验证集固定**：继续保持你的 772 exams / 1538 ears 固定验证集，这对可复现很关键

  * 但要补上：**对每个 task 单独统计 val 正例数**，尤其是 code4（只有 8 正例耳朵），后面必须给 CI

### 1.3 数据 QA：你现在最需要的三张表

1. **扫描参数分布**（train vs val vs date_match=False）：

   * spacing、slice thickness、kernel、机型厂商
   * 目的：判断域差/噪声来源是否集中在某些参数上

2. **每类的“切片范围/覆盖”分布**：

   * 你现在均匀采 32 张，很可能把病灶层面稀释掉
   * 用 `n_instances`、z 方向覆盖长度等做对比

3. **高风险样本列表**：

   * `date_match=False` + 模型训练时持续高 loss + 预测极不稳定
   * 后面用于“半自动标注复核/清洗”

---

## 2. 任务设计：把 6 分类拆成“可控的临床任务集合”

你已经决定拆成多组二分类，这是正确方向。建议你把每个 task 明确成两种口径（临床上常用）：

### 2.1 每个二分类任务给两种定义

* **Pairwise（严格两类对比）**：只取 A 类 vs B 类，其余类别全部丢弃

  * 优点：任务清晰、噪声少
  * 缺点：样本量减少

* **One-vs-Rest（更贴近真实筛查）**：A 类为正，其余所有（或“其余异常”）为负

  * 优点：更接近落地场景
  * 缺点：负类更杂，难度更高，解释要更谨慎

### 2.2 推荐你优先做的任务顺序（从最稳到最难）

按你的类别数量（耳朵）估计难度：

1. **正常（code5） vs 非正常（1,2,3,4,6）**

   * 正例 ~5478，负例 2214（整体不缺数据）
   * 这是“表征学习”的最佳主任务（后面所有任务都收益）

2. **胆脂瘤（code2） vs 正常（code5）**（Pairwise）

   * 正例 1156，负例 2214，样本够、临床价值高

3. **慢性化脓性中耳炎（code1） vs 正常（code5）**

4. **分泌性中耳炎（code3） vs 正常（code5）**

5. **胆脂瘤（code2） vs 其他异常（1,3,4,6）**（更贴近鉴别诊断）

6. **任何包含 code4（胆固醇肉芽肿）作为正例的任务**

   * 总共 36 耳，训练正例 28、验证正例 8
   * 这个必须按 **few-shot 规范**做（见第 7 节），否则指标没有意义

> 关键点：把 code4 任务从“常规监督学习任务”中剥离出来，单独用 few-shot/embedding 方法做，并且强制报告 CI 和不稳定性。

---

## 3. 输入与预处理：把“中线一刀切 + 均匀32切片”升级为更可靠的 ROI 与采样

你现在的处理方式能跑通，但从信号角度存在三个大坑：**左右分割误差、关键解剖区域未对齐、z 方向信息被均匀稀释**。建议按“投入产出比”分三档升级。

### 3.1 必做：几乎零成本但收益很大的改动

1. **窗宽窗位 jitter（强烈建议）**

   * 你现在固定 500/3000。训练时随机扰动 WL/WW（例如 WL±200，WW×[0.8,1.2]）
   * 目的：增强对扫描协议/重建核差异的鲁棒性

2. **强制统一 slice 排序与方向**

   * 用 `ImagePositionPatient` 排序，不要依赖文件名
   * 用 `ImageOrientationPatient` 做方向统一（至少保证左右翻转后方向一致）

3. **把“耳朵作为样本单位”训练**

   * 你现在一次喂 (2,1,32,224,224)。建议训练时改成 ear-level batch：`(B,1,32,H,W)`
   * 左右共享同一网络即可；缺失侧直接不进 batch
   * 好处：

     * batch 变大、采样更灵活（尤其少数类）
     * 每个二分类任务更自然（不用双输出 mask）

### 3.2 强烈建议：ROI 更精准 + z 采样更“看病灶”

#### A) 左右分割：从“图像中线”变成“骨结构对称轴”

* 在骨窗下（HU>300）得到骨 mask
* 计算骨 mask 的 x 方向投影，取重心/对称轴，得到更稳的“中线”
* 用这个中线切分左右半幅（比固定宽度一刀切鲁棒得多）

#### B) z 采样：从“全序列均匀采样”变成“锁定颞骨区域的连续块”

你想要的是“中耳/乳突区域那一段”。给你两种不需要标注的做法：

**方法 1（启发式）**：骨+气房引导

* 对每一 slice：

  * 在骨 mask 内统计低 HU（例如 HU<-500）的比例（代表气房/含气结构）
* 找到该比例最大的区域（或平滑后峰值），以此为中心取连续 `K` 张（例如 K=64），再从中采 32 张
* 这样比全序列均匀采样更容易覆盖病灶相关层面

**方法 2（可学习但轻量）**：slice scorer（两阶段 MIL 思路）

* 先用低分辨率（例如 128×128）训练一个 slice-level “相关性评分器”（不需要额外标注，用耳标签做 MIL）
* 每次从全序列挑 top-K 切片/连续块再喂主分类器
* 这个方法对“扫描范围不一致”特别有效

#### C) 空间 crop：从“半幅全图”到“颞骨 patch”

左右半幅里很多是无关结构。建议在每侧再做一次粗定位 crop：

* 在每侧图里用骨 mask 找最大连通块附近的外侧区域（靠近图像边缘的颞骨）
* 取固定大小 patch（例如 160~192 的正方形），再 resize 到 224
* **收益：**更聚焦 ROI、减轻模型容量浪费、提高泛化

> 你可以先做启发式 crop，后面再考虑更复杂的定位网络。

### 3.3 如果你愿意再进一步：统一物理尺度

不同 spacing/thickness 会让“同样 224×224”代表的实际毫米范围不同，模型会混乱。

* 更理想做法：把每个 ear volume resample 到统一 spacing（例如 0.6~0.8mm 的近似等距/或至少 z spacing 统一到某个范围）
* 工程上可做离线预处理+缓存（zarr/npz）避免每次训练重采样

---

## 4. 模型路线：优先用“2D 编码器 + 序列聚合”取代重 3D（更稳、更省显存、更可解释）

你当前 3D 输入 (32,224,224) 很大，纯 3D backbone 容易 OOM / batch 太小 / 不稳定。推荐你用一个在医学 CT 上非常常见的折中：

### 4.1 推荐主干：Slice Encoder（2D）+ Z 方向聚合（Attention/Transformer）

**结构：**

1. 每个 slice 用同一个 2D CNN 编码（ResNet18/EfficientNet-B0/ConvNeXt-Tiny 这类都行）

2. 得到每个 slice 一个 embedding（例如 512 维）

3. 用 z 方向聚合模块把 32 个 embedding 聚成一个 ear embedding：

   * baseline：mean pooling / max pooling
   * 更强：attention pooling（学习每片权重）
   * 更强但仍轻：1-2 层 transformer encoder（加位置编码）

4. 分类头：二分类就是一个线性层输出 logit（或小 MLP + dropout）

**优点：**

* 显存小、batch 可做大、训练稳定
* attention 权重直接告诉你“关键切片在哪”（天然可解释）
* 可以做 TTA：换不同 slice block 预测再平均

### 4.2 双耳共享：用同一套网络，左右当两个样本

* 训练时 ear-level batch：左右耳分别过同一个网络
* 如果你坚持“一个检查同时输入左右耳”：也可以把左右作为 batch 维度拼起来，本质一样

### 4.3 如果你一定要 3D：给一个“能跑稳”的版本

* 输入先做 ROI crop 到 160 或 128
* backbone 用轻量 3D（3D-ResNet18 或 R(2+1)D 类）
* 必须 AMP + grad accumulation + 强正则
* 但我仍建议你先把 2D+聚合打稳，再决定是否值得上 3D

---

## 5. 训练策略：针对“不均衡 + 噪声 + 小样本”的统一配方

### 5.1 采样与 loss（核心）

对二分类任务，建议你用这一套“默认组合”，然后再按 task 微调：

1. **Balanced batch（强烈建议）**

* 每个 batch 里固定正负比例，例如 1:1 或 1:3
* 用 `WeightedRandomSampler` 或自定义 batch sampler
* 这样比单纯给 loss 加权更稳（尤其 batch 小时）

2. **BCEWithLogits + pos_weight**

* `pos_weight = N_neg / N_pos`（可稍微 clip，比如最大不超过 20）
* 对正例极少的 task（如 code4），pos_weight 会爆炸，这时要换策略（见第 7 节）

3. **Focal Loss（可选，用在难任务）**

* 当你发现模型对负类过于自信、正类召回上不来时再用
* 但 focal 也会放大噪声样本影响，所以最好搭配 robust 策略（下面）

4. **Label smoothing / bootstrapping（抗噪）**

* CT 分类很容易被少量错配/噪声拖着跑
* 二分类可做：`y = (1-ε)*y + ε*0.5`（例如 ε=0.05~0.1）
* 或者对高 loss 样本做逐步降权（“小损失优先”思想）

### 5.2 数据增强（CT 专用建议）

不要照搬自然图像的颜色增强，但可以用这些：

* 随机 WL/WW（前面提过，收益大）
* 小角度旋转（±7°以内）、平移、缩放
* 轻微高斯噪声/模糊（模拟重建差异）
* 随机裁剪再 resize（但要确保 ROI 不被裁掉，最好在 crop 后做）
* **不要**做大幅度形变（会破坏解剖合理性）

### 5.3 优化器与训练细节（让你跑得稳）

* 优化器：AdamW（weight decay 1e-4 左右）
* 学习率：用 cosine decay + warmup（warmup 5% step）
* AMP 混合精度（必须）
* 梯度裁剪（例如 1.0）
* 早停：以 **AUPRC 或 sensitivity@high-specificity** 为主（比 accuracy 更靠谱）

### 5.4 小数据比例实验（1%/20%）怎么做才公平

* **先冻结 2D backbone**，只训练聚合层 + 头（linear probe）
* 然后再全量微调（fine-tune）
* 这样在 1% 数据时不至于完全崩掉，而且更符合“可训练信号有限”的现实

---

## 6. “6 分类”如果你还想保留：建议走两阶段/分层，而不是硬 6-way

在你当前数据下，code4 直接把 macro 指标拖垮几乎是必然的。更合理的 6 类路线是：

### 6.1 分层两阶段（推荐）

* Stage 1：正常（5） vs 异常（1/2/3/4/6）
* Stage 2：异常再细分

  * 先做 2 vs 非2（胆脂瘤鉴别）
  * 再做 1 vs 3 vs 6（把 4 暂时并入 6 或单独 few-shot）

这样做的好处是：

* 第一阶段把“正常筛查”做得很稳
* 第二阶段在异常集合里学习更细粒度差异
* code4 不至于让整个模型训练目标崩坏

### 6.2 code4 的处理建议

* 主模型里把 code4 合并到 “其他(6)”（或异常-其他）
* code4 单独做 few-shot 检测/提示（见下一节），并且在报告里明确说明“证据强度有限”

---

## 7. code4（胆固醇肉芽肿）专用方案：按 few-shot 规范来，否则指标不可信

你已经指出本质：训练正例 28、验证正例 8。
这类任务常见正确打法不是“端到端深网二分类”，而是：

### 7.1 先学一个“通用耳部 CT 表征”（用所有数据）

用你最稳的任务当表征学习主任务（推荐：正常 vs 异常），训练一个 embedding 网络：

* 网络：第 4 节的 2D+聚合模型
* 训练目标：

  * 主任务 BCE（正常/异常）
  * +（可选）Supervised Contrastive / Triplet（同类拉近、异类拉远）
* 结果：得到一个对耳部病变更泛化的 embedding space

### 7.2 在 embedding 上做“轻模型 few-shot 分类”

对 code4：

* 从 embedding 中取每个耳朵的向量
* 用以下任一方法做分类（通常比端到端更稳）：

  1. **原型分类（Prototypical）**：code4 的均值向量作为 prototype，看距离
  2. **正则化 Logistic / 线性 SVM**：L2 强正则，避免过拟合
  3. **kNN 检索式诊断**：给出最相似的训练样本作为解释（临床沟通更友好）

### 7.3 评估必须这样做（否则方差巨大）

* **不要只报一个点估计**
* 至少做：

  * bootstrap（按 exam 或 ear 抽样）得到 AUROC/AUPRC 的 95% CI
  * sensitivity/specificity 的 CI
* 并且明确写：验证正例只有 8，任何单次结果波动都正常

> 这一步的意义：你把 code4 从“训练不动”变成“有可解释的相似病例提示 + 带置信区间的风险评分”，这才是合理产出。

---

## 8. 噪声与域差：用“稳健训练 + 可疑样本回溯”闭环解决

你提到 `date_match=False` 很多，这很可能意味着：

* 标注与影像存在一定错配风险
* 或者来源批次不同带来域差（不同扫描时间/协议）

建议做一个非常实用的闭环：

### 8.1 训练侧：抗噪组合

* label smoothing（ε=0.05~0.1）
* 小损失优先/样本降权：对持续高 loss 样本逐步降权（不要直接删）
* 强化 WL/WW jitter + 轻噪声增强（抗域差）

### 8.2 分析侧：可疑样本列表（供人工复核）

每个 epoch（或最终模型）输出：

* 高置信度预测但与标签相反的样本
* 长期 top-K loss 的样本
* 且按 `date_match=False` 分组

你不需要立刻人工全复核，但你会很快发现“噪声集中在哪些类型/哪些目录/哪些时间段”，然后再决定修数据还是做 domain-specific 处理。

---

## 9. 评估协议：让结果“稳定、可比较、能写进报告”

### 9.1 指标：不要只盯 AUROC

对二分类临床任务，建议固定输出以下集合：

* AUROC（看整体排序能力）
* **AUPRC**（正例比例不稳定时更敏感）
* Sensitivity / Specificity（按固定阈值）
* **Sensitivity@95%Specificity**（或反过来，取决于你临床偏好）
* F1（但注意它受阈值影响很大）

### 9.2 阈值选择：避免“用验证集调阈值”污染

推荐两种做法（二选一）：

* **固定阈值 0.5**：最干净，适合研究对比
* **在训练集内做阈值选择**：比如用 train 的交叉验证或留出一小块 calibration set，只用于阈值/温度标定，然后在固定 val 上报告

### 9.3 置信区间：强制做（尤其 code4）

* bootstrap（建议按 **exam_id** 抽样，避免左右耳相关性导致 CI 偏乐观）
* 输出 95% CI（AUROC/AUPRC/敏感度/特异度）

### 9.4 稳定性：至少 3 个 seed

* 每个任务跑 3 个 seed
* 报告 mean ± std + CI（CI 对 val 的 bootstrap，std 对训练随机性）

---

## 10. 可解释性：从“能看 CAM”升级到“能看关键切片 + 相似病例”

你要的可解释，建议分三层：

### 10.1 Slice-level 解释（你用 attention pooling 时天然具备）

* 输出每个 slice 的权重
* 展示 top-3 权重切片（临床非常直观）

### 10.2 CAM/Grad-CAM（对 2D slice encoder 很好做）

* 对 top slice 做 CAM 热力图
* 叠加到骨窗图像上，给出模型关注区域

### 10.3 Case-based（检索式解释）

在 embedding 空间做最近邻：

* 对一个预测为 cholesteatoma 的样本，给出训练集中最相似的 3 个 cholesteatoma 例子
* 这对医生接受度非常高，也能帮助你排查错标/域差

---

## 11. 工程：让 GPU 不再空转（你已经在做 cache，但建议做到“离线标准化”）

你当前瓶颈是 DICOM 解码吞吐 vs GPU 利用率。建议做到：

### 11.1 离线预处理产物（一次做完，多次复用）

每个 ear 样本预先保存：

* `uint8/float16` 的 `(32, H, W)` 或 `(K, H, W)` volume（已经 WL/WW + normalize + crop + 对齐）
* metadata（spacing、slice index、原始路径等）

存储格式建议：

* npz（简单）/ zarr（大规模、并行读写友好）/ memmap（极快）

### 11.2 DataLoader 的稳定高吞吐配置

* `persistent_workers=True`
* `pin_memory=True`
* `prefetch_factor` 合理调大
* worker 里不要做重 CPU 操作（都离线了）

---

## 12. 最终推荐的“落地路线”（不需要你一次性做完全部）

如果你要一个最稳的执行顺序（从高收益/低风险开始）：

1. **耳朵级样本化 + manifest + QA 表**（立刻做）
2. **2D slice encoder + attention pooling baseline**（先把 pipeline 打稳）
3. **任务优先级：正常vs异常 → 胆脂瘤vs正常 → 其他**
4. **z 采样从均匀 → 连续块（启发式）**
5. **左右分割从固定中线 → 骨对称轴中线**
6. **置信区间 + 多 seed 稳定性报告（把实验变成“可信结果”）**
7. **code4：embedding + few-shot 轻模型 + 检索式解释（单独报告）**
8. （可选）再考虑 3D / resample spacing / MIL slice scorer

---

## 你会得到什么“最终形态”的产出

* 一套统一的训练框架：

  * 同一 backbone + 聚合器
  * 多个二分类 head（每个 task 一个 head 或按需单独训练）
* 每个 task 都有：

  * 固定验证集上的 AUROC/AUPRC/敏感度/特异度/F1
  * bootstrap 置信区间
  * 3-seed 稳定性
  * 关键切片 + CAM 可解释输出
* code4 有：

  * few-shot 风格的风险评分 + 近邻相似病例提示 + 明确的 CI

---

如果你希望我把这套方案进一步“工程化到可以直接实现”，我可以按你目前代码形态（你用的是 PyTorch 还是其他）把下面三样写成更具体的清单/伪代码级别：

1. manifest 字段设计 + task 过滤/映射逻辑
2. 2D+attention 聚合模型的模块拆分（含双耳/耳级 batch）
3. 评估脚本：bootstrap CI、按 exam 抽样、阈值选择与报告模板

你不需要再补充信息，我也可以先按你给的数据结构默认给出一个“标准实现骨架”。
